<!--HTML header for doxygen 1.8.14-->
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Doxygen 1.8.11"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Unity Modules: Interaction Engine</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab" rel="stylesheet"> 
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="extra.css" rel="stylesheet" type="text/css"/>
<!-- Custom resize overrides -->
<script type="text/javascript" src="resizeoverrides.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><a href="index.html"><img alt="Logo" src="LeapMotion_Logo_White_Small.png"/></a></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Unity Modules
   </div>
   <div id="projectbrief">Leap Motion&#39;s Unity SDK.</div>
   <div id="projectnumber">4.4.0</div>
  </td>
   <td id="searchbar">        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<div id="searchboxrect"></div>
<!-- end header part
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('interaction-engine.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Interaction Engine </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The Interaction Engine allows users to work with your VR application by interacting with <em>physical</em> or <em>pseudo-physical</em> objects. Whether a baseball, a <a href="https://www.youtube.com/watch?v=oZ_53T2jBGg&amp;t=1m11s" title="Leap Motion Blocks demo">block</a>, a virtual trackball, a button on an interface panel, or a hologram with more complex affordances, if there are objects in your application you need your user to be able to <b>hover near</b>, <b>touch</b>, or <b>grasp</b> in some way, the Interaction Engine can do some or all of that work for you.</p>
<p>You can find latest stable Interaction Engine package <a href="https://developer.leapmotion.com/unity" title="Leap Motion Unity Developer site">on our developer site</a>.</p>
<p><b>The Interaction Engine requires Unity 5.6 or later.</b></p>
<p>If you take a look at the examples that come with the package, you'll notice some common patterns that are good to follow when building things with the Interaction Engine.</p>
<h1><a class="anchor" id="ie-basic-components"></a>
The basic components of interaction</h1>
<ul>
<li>"Interaction objects" are GameObjects with an attached InteractionBehaviour. They require a Rigidbody and at least one Collider.</li>
<li>The InteractionManager receives FixedUpdate from Unity and handles all the internal logic that makes interactions possible, including updating hand/controller data and interaction object data. <b>You need one of these in your scene for interaction objects to function!</b> A good place for it is right underneath your player's top-level camera rig object (not the camera itself, but its parent).</li>
<li>Each InteractionController does all the actual <em>interacting</em> with interaction objects, whether by picking them up, touching them, hitting them, or just being near them. This object could be the user's hand by way of the InteractionHand component, or a VR Controller (e.g. Oculus Touch or Vive controller) if it uses the InteractionXRController component. Interaction controllers <b>should sit beneath the Interaction Manager in the hierarchy</b> &ndash; see the diagram below.</li>
</ul>
<div class="image">
<img src="http://blog.leapmotion.com/wp-content/uploads/2017/07/IESetupDiagram.png" />
</div>
<p>Interaction objects may or may not live inside the player's camera rig depending on whether or not they should move with the player. Interaction controllers, <em>including</em> Interaction Hands, always live underneath the Interaction Manager, and the Interaction Manager should be a sibling of the camera object.</p>
<h1><a class="anchor" id="ie-just-add-interactionbehaviour"></a>
Just add InteractionBehaviour!</h1>
<p>When you add an <a class="el" href="class_leap_1_1_unity_1_1_interaction_1_1_interaction_behaviour.html">InteractionBehaviour</a> component to an object, a couple of things happen automatically:</p>
<ul>
<li>If it didn't have one before, the object will gain a <a href="https://docs.unity3d.com/ScriptReference/Rigidbody.html">Rigidbody</a> component with gravity enabled, making it a physically-simulated object governed by Unity's PhysX engine. If your object doesn't have a <a href="https://docs.unity3d.com/ScriptReference/Collider.html">Collider</a>, it will fall through the floor!</li>
<li>Assuming you have an Interaction Manager with one or more interaction controllers beneath it, you'll be able to pick up, poke, and smack the object with your hands or VR controller.</li>
</ul>
<p>The first example in the Interaction Engine package showcases the default behavior of a handful of different objects when they first become interaction objects.</p>
<h1><a class="anchor" id="ie-first-steps"></a>
First steps with the Interaction Engine</h1>
<p>If you haven't already, import <a class="el" href="core.html">Core</a> and the Interaction Engine into your Unity project:</p>
<ul>
<li>Download the latest Core package from <a href="https://developer.leapmotion.com/unity" title="Leap Motion Unity Developer site">our developer site</a>.</li>
<li>Download the latest Interaction Engine package from <a href="https://developer.leapmotion.com/unity" title="Leap Motion Unity Developer site">our developer site</a>.</li>
<li>Import both packages. To import a <code>.unitypackage</code> file, double-click on it while your project is open, or go to <code>Assets -&gt; Import Package...</code> and choose the package.</li>
<li>If you see errors, make sure your project has the latest version of Core (one section up), and that your project was opened using Unity 5.6 or later.</li>
</ul>
<h2>Update the Physics timestep and gravity!</h2>
<p>Unity's physics engine has a "fixed timestep," and that timestep is not always in sync with the graphics frame rate. It is very important that you set the physics timestep to be the same as the rendering frame rate. If you are building for an Oculus or Vive, this means that your physics timestep should be <code>0.0111111</code> (corresponding to 90 frames per second). This is configured via <code>Edit -&gt; Project Settings -&gt; Time</code>. For more details on this process, visit [[Scripting Interaction Objects | Scripting Interaction Objects]]</p>
<p>Additionally, we've found that setting your gravity to half its real-world scale (-4.905 on the Y axis instead of -9.81) produces a better feeling when working with physical objects. We strongly recommend setting your gravity in this way; you can change it in <code>Edit -&gt; Project Settings -&gt; Physics</code>.</p>
<h2>Get your XR rig ready</h2>
<p>If you don't already have a Leap-enabled XR camera rig to your scene, you can follow these steps:</p><ul>
<li>Open a new scene and delete the <code>Main Camera</code> object. (We'll set up our own.)</li>
<li>Drag the <b>Leap Rig</b> prefab into your scene: <code>LeapMotion/Core/Prefabs</code>.</li>
<li>Drag the <b>Interaction Manager</b> prefab into your camera rig: <code>LeapMotion/Modules/Interaction/Prefabs</code>.</li>
</ul>
<p>If you aren't familiar with Leap-enabled XR rigs, check out <a class="el" href="core.html#xr-rig-setup">Leap Motion XR rigs</a>.</p>
<p>It is possible to use a custom camera rig in combination with Leap Motion. If you'd like to use something other than the <b>Leap Rig</b> prefab, you should make sure you have a camera tagged MainCamera in your scene, and that it has children with the same components and linkages that you can find beneath the Camera object in the <b>Leap Rig</b> prefab. Note that the Interaction Engine on its own does not render hands, it only instantiates physical representations of hands.</p>
<p>Generally, it's a good idea to keep your scene organized. At Leap, we tend to put player-centric scripts in GameObjects as siblings of the Main Camera object. For example, the AttachmentHands script offers a convenient way to attach arbitrary objects to any of the joints in a Leap hand representation, and it belongs in such a sibling GameObject. To create AttachmentHands for use in your scene, you would:</p><ul>
<li>Create a new GameObject in your scene</li>
<li>Rename it <code>Attachment Hands</code></li>
<li>Drag it into the Rig object so that it sits beneath your Rig object</li>
<li>Add the AttachmentHands script to it: drag the <code>LeapMotion/Core/Scripts/Attachments/AttachmentHands.cs</code> script onto the object, or use the AddComponent menu with the object selected and type <code>AttachmentHands</code>.</li>
</ul>
<h2>Configure InteractionXRControllers for grasping</h2>
<p>If you intend to use the Interaction Engine with Oculus Touch or Vive controllers, you'll need to configure your project's input settings before you'll be able to use the controllers to grasp objects. Input settings are project settings that cannot be changed by imported packages, which is why we can't configure these input settings for you. You can skip this section if you are only interested in using Leap hands with the Interaction Engine.</p>
<p>Go to your Input Manager (<code>Edit-&gt;Project Settings-&gt;Input</code>) and set up the joystick axes you'd like to use for left-hand and right-hand grasps. (Controller triggers are still referred to as 'joysticks' in Unity's parlance.) Then make sure each InteractionVRController has its grasping axis set to the corresponding axis you set up. The default prefabs for left and right InteractionVRControllers will look for axes named <code>LeftVRTriggerAxis</code> and <code>RightVRTriggerAxis</code>, respectively.</p>
<p>Helpful diagrams and axis labels can be found in <a href="https://docs.unity3d.com/Manual/OpenVRControllers.html">Unity's documentation</a>.</p>
<h1><a class="anchor" id="ie-examples"></a>
Check out the examples</h1>
<p>The examples folder (<code>LeapMotion/Modules/Interaction/Examples</code>) contains a series of example scenes that demonstrate the features of the Interaction Engine.</p>
<p>Most of the examples can be played using Leap hands via the Leap Motion Controller <em>or</em> using any VR controller that Unity provides built-in support for, such as Oculus Touch controllers or Vive controllers.</p>
<h2>Example 1: Interaction Objects 101</h2>
<div class="image">
<img src="http://blog.leapmotion.com/wp-content/uploads/2017/06/InteractionObjectsExample.gif" />
</div>
<p>The Interaction Objects example shows the default behavior of interaction objects when they first receive their InteractionBehaviour component.</p>
<p>Reach out with your hands or your VR controller and play around with the objects in front of you to get a sense of how the default physics of interaction objects feels. In particular, you should notice that objects don't jitter or explode, even if you attempt to crush them or pull on the constrained objects in various directions.</p>
<p>On the right side of this scene are floating objects that have been marked <b>kinematic</b> and that have <code>ignoreGrasping</code> and <code>ignoreContact</code> set to <code>true</code> on their InteractionBehaviours. These objects have a simple script attached to them that causes them to glow when hands are nearby &ndash; but due to their interaction settings, they will only receive hover information, and cannot be grasped. Note that Rigidbodies collide against these objects even though they have <code>ignoreContact</code> set to true &ndash; this setting applies only against interaction controllers, not for arbitrary Rigidbodies.</p>
<h2>Example 2: Basic UI in the Interaction Engine</h2>
<div class="image">
<img src="http://blog.leapmotion.com/wp-content/uploads/2017/06/BasicUIExample.gif" />
</div>
<p>Interacting with interface elements is a very particular <em>kind</em> of interaction, but in VR, we find these interactions to make the most sense to users when they are provided physical metaphors and familiar mechanisms. Thus, we've built a small set of fine-tuned InteractionBehaviours (that will continue to grow!) that deal with this extremely common use-case: The InteractionButton, and the InteractionSlider.</p>
<p>Try manipulating this interface in various ways, including ways that it doesn't expect to be used. You should find that even clumsy users will be able to push only one button at a time: Fundamentally, <b>user interfaces in the Interaction Engine only allow the 'primary hovered' interaction object to be manipulated or triggered at any one time</b>. This is a soft constraint; primary hover data is exposed through the InteractionBehaviour API for any and all interaction objects for which <b>hovering</b> is enabled, and the InteractionButton script enforces the constraint by disabling contact when it is not 'the primary hover' of an interaction controller.</p>
<h2>Example 3: Interaction Callbacks for Handle-type Interfaces</h2>
<div class="image">
<img src="http://blog.leapmotion.com/wp-content/uploads/2017/06/InteractionCallbacksExample.gif" />
</div>
<p>The Interaction Callbacks example features a set of interaction objects that collectively form a basic Transform Tool the user may use at runtime to manipulate the position and rotation of an object. These interaction objects ignore contact, reacting only to grasping controllers and controller proximity through hovering. Instead of allowing themselves to be moved directly by grasping hands, these objects report information to their managing TransformTool object, which orchestrates the overall motion of the target object and each handle at the end of every frame.</p>
<h2>Example 4: Attaching Interfaces to the User's Hand</h2>
<div class="image">
<img src="http://blog.leapmotion.com/wp-content/uploads/2017/06/HandUIExample.gif" />
</div>
<p>Simple applications may want to place an interface directly attached to the user's hands so core functionalities are always within arm's reach. This example demonstrates that concept by animating one such interface into view when the user looks at their own palm (or the belly of their VR controller; this may be better mapped to a VR controller button!).</p>
<h2>Example 5: Building on Interaction Objects with Anchors</h2>
<div class="image">
<img src="http://blog.leapmotion.com/wp-content/uploads/2017/06/AnchorsExample.gif" />
</div>
<p>The AnchorableBehaviour, Anchor, and AnchorGroup components constitute an optional set of scripts that are included with the Interaction Engine that build on the basic interactivity afforded by interaction objects. This example demonstrates all three of these components. AnchorableBehaviours integrate well with InteractionBehaviour components (they are designed to sit on the same GameObject) and allow an interaction object to be placed in Anchor points that can be defined anywhere in your scene.</p>
<h2>Example 6: Dynamic Interfaces: Interaction Objects, AttachmentHands, and Anchors</h2>
<div class="image">
<img src="http://blog.leapmotion.com/wp-content/uploads/2017/06/DynamicUIExample.gif" />
</div>
<p>InteractionButtons and InteractionSliders are useful on their own, but they become truly powerful tools in your UI toolkit when combined with Anchors, and Core utilities like the AttachmentHands and the Tween library to allow the user to carry around entire physical interfaces on their person in VR. This example combines all of these components to demonstrate using the Interaction Engine to build a set of portable VR interfaces.</p>
<h2>Example 7: Moving Reference Frames</h2>
<p>The Interaction Engine keeps your interfaces working even while the player is being translated and rotated. Make sure your player moves during FixedUpdate, before the Interaction Engine performs its own FixedUpdate. Also make sure the Interaction Manager object moves with the player &ndash; this is most easily accomplished by placing it beneath your player object in the hierarchy. This example demonstrates a working configuration you can reference for your own application.</p>
<h2>Example 8: Swap Grasp</h2>
<p>This example scene demonstrates the use of the <a class="el" href="class_leap_1_1_unity_1_1_interaction_1_1_interaction_controller.html">InteractionController</a>'s SwapGrasp() method, which allows you to instantly swap an object that the user is holding for another. This is especially useful if you need objects to morph while the user is holding them.</p>
<h1><a class="anchor" id="ie-working-with-physx"></a>
Working with PhysX objects in Unity</h1>
<p>Before scripting behavior with the Interaction Engine, you should know the basics of working with PhysX Rigidbodies in Unity. Most importantly, you should understand Unity's physics scripting execution order:</p>
<ol type="1">
<li>FixedUpdate (user physics logic) <b>sometimes, but always with PhysX</b></li>
<li>PhysX updates Rigidbodies and resolves collisions <b>sometimes, but always with FixedUpdate</b></li>
<li>Update (user graphics logic) <b>once every frame</b></li>
</ol>
<p>Source: <a href="https://docs.unity3d.com/uploads/Main/monobehaviour_flowchart.svg">this helpful chart from Unity</a>, via <a href="https://docs.unity3d.com/Manual/ExecutionOrder.html">the execution order page</a>.</p>
<p><b>FixedUpdate</b> happens just before the physics engine "PhysX" updates and is where user physics logic goes! This is where you should modify the positions, rotations, velocities, and angular velocities of your Rigidbodies to your liking before the physics engine <em>does physics to them</em>.</p>
<p><b>FixedUpdate may happen 0 or more times per Update.</b> VR applications usually run at 90 frames per second to avoid sickening the user. Update runs once before the Camera in your scene renders what it sees to the screen or your VR headset. Unity's physics engine has a "fixed timestep" that is configured via Edit-&gt;Project Settings-&gt;Time. Here at Leap, we build applications with a fixed timestep of <code>0.0111111</code> to try and get a FixedUpdate to run once a frame, and this is the setting we recommend. But do note that FixedUpdate <b>is not</b> guaranteed to fire before every rendered frame, if your time-per-frame is less that your fixed timestep. Additionally, FixedUpdate may happen two or more times before a rendered frame: this will happen if you spend more than two fixed timesteps' worth of time on any one render frame (i.e. if you "drop a frame" because you tried to do too much work during one Update or FixedUpdate).</p>
<p>Naturally, because the Interaction Engine deals entirely in physics objects, <b>all interaction object callbacks occur during FixedUpdate</b>. While we're on the subject of potential gotchas, here are a few more gotchas when working with physics:</p>
<ul>
<li>The update order (FixedUpdate, PhysX, Update) implies that if you move physics objects via their Rigidbodies during Update and not during FixedUpdate, the new positions/rotations will not be visible until the <em>next</em> update cycle, after the physics engine manipulates objects' Transforms via their Rigidbodies.</li>
<li>When you move a PhysX object (Rigidbody) via its Transform (<code>transform.position</code> or <code>transform.rotation</code>) <em>instead of its Rigidbody</em> (<code>rigidbody.position</code> or <code>rigidbody.rotation</code>), you <b>force PhysX to immediately do some heavy recalculations internally</b>, so if you do this a lot to a bunch of physics objects a frame, it could be bad news for your framerate. Generally, we don't recommend doing this! (But sometimes it's necessary.)</li>
</ul>
<h1><a class="anchor" id="ie-custom-layers"></a>
Custom layers for interaction objects</h1>
<p>Have a custom object layer setup? No problem. Interaction objects need to switch between two layers at runtime: One to exist on when the object <b>can</b> collide with your hands (think "contact enabled"), and one to exist on when the object <b>can't</b> collide with your hands (When "contact disabled", and also when <b>grasping</b> the object).</p>
<p>On a specific Interaction Behaviour under its <b>Layer Overrides</b> header, check <code>Override Interaction Layer</code> and <code>Override No Contact Layer</code> in its inspector to specify custom layers to use for the object when contact is enabled or disabled (e.g. due to being grasped). These layers must follow collision rules with respect to the <b>contact bone layer</b>, which is the layer that contains the Colliders that make up the bones in Interaction Hands or Interaction Controllers. (The contact bone layer is usually automatically generated, but you can specify a custom layer to use for Interaction Controllers in the Interaction Manager's inspector.) The rules are as follows:</p>
<ul>
<li>The Interaction Layer should have collision enabled with the contact bone layer.</li>
<li>The No Contact layer should <b>NOT</b> have collision enabled with the contact bone layer.</li>
<li>(Any collision configuration is allowed for these layers with respect to any non-contact-bone layers.)</li>
</ul>
<p>You can override both or only one of the layers for interaction objects as long as these rules are followed.</p>
<h1><a class="anchor" id="ie-custom-interaction-behaviors"></a>
Custom behaviors for interaction objects</h1>
<p>Be sure to take a look at examples 2 through 6 to see how interaction objects can have their behavior fine-tuned to meet the specific needs of your application. The standard workflow for writing custom scripts for interaction objects goes something like this:</p>
<ul>
<li>Be sure your object has an InteractionBehaviour component (or an InteractionButton or InteractionSlider component, each of which inherit from InteractionBehaviour).</li>
<li>Add your custom script to the interaction object and initialize a reference to the InteractionBehaviour component. <div class="fragment"><div class="line"><span class="keyword">using</span> <a class="code" href="namespace_leap.html">Leap</a>.<a class="code" href="namespace_leap_1_1_unity.html">Unity</a>.<a class="code" href="namespace_leap_1_1_unity_1_1_interaction.html">Interaction</a>;</div><div class="line"><span class="keyword">using</span> <a class="code" href="namespace_unity_engine.html">UnityEngine</a>;</div><div class="line"></div><div class="line">[RequireComponent(typeof(InteractionBehaviour))]</div><div class="line"><span class="keyword">public</span> <span class="keyword">class </span>CustomInteractionScript : MonoBehaviour {</div><div class="line"></div><div class="line">  <span class="keyword">private</span> InteractionBehaviour _intObj;</div><div class="line"></div><div class="line">  <span class="keywordtype">void</span> Start() {</div><div class="line">    _intObj = GetComponent&lt;InteractionBehaviour&gt;();</div><div class="line">  }</div><div class="line"></div><div class="line">}</div></div><!-- fragment --></li>
<li>Check out the API documentation (or take advantage of IntelliSense!) for the InteractionBehaviour class to get a sense of what behavior you can control through scripting, or look at the examples below.</li>
</ul>
<h2>Disabling/enabling interaction types at runtime</h2>
<p>Disabling and enabling <b>hover</b>, <b>contact</b>, or <b>grasping</b> at or before runtime is a first-class feature of the Interaction Engine. You have two ways to do this:</p>
<h3>Option 1: Using controller interaction types</h3>
<p>The InteractionController class provides the <code>enableHovering</code>, <code>enableContact</code>, and <code>enableGrasping</code> properties. Setting any of these properties to false will immediately fire "End" events for the corresponding interaction type and prevent the corresponding interactions from occurring <b>from this controller towards any interaction object</b>.</p>
<h3>Option 2: Using object interaction overrides</h3>
<p>The InteractionBehaviour class provides the <code>ignoreHover</code>, <code>ignoreContact</code>, and <code>ignoreGrasping</code> properties. Setting any of these properties to true will immediately fire "End" events for the corresponding interaction type (for this object only) and prevent the corresponding interactions from occurring <b>from any controller towards this interaction object</b>.</p>
<h2>Constraining an object's held position and rotation</h2>
<h3>Option 1: Use PhysX constraints</h3>
<p>The Interaction Engine will obey the constraints you impose on interaction objects whose Rigidbodies you constrain using <a href="https://docs.unity3d.com/Manual/Joints.html">Joint</a> components. If you grasp a <em>non-kinematic</em> interaction object that has a Joint attached to it, the object will obey the constraints imposed by that joint.</p>
<p><b>If you add or remove an interaction object's Joints at runtime</b> and your object is graspable, you should call <code>_intObj.RefreshPositionLockedState()</code> to have the object check whether any attached Joints or Rigidbody state lock the object's position. Under these circumstances, the object must choose a different grasp orientation solver to give intuitively correct behavior. Check [[ the API documentation on this method | <a href="https://developer.leapmotion.com/documentation/unity/class_leap_1_1_unity_1_1_interaction_1_1_interaction_behaviour.html#a33f9f48f2c6375cb926cc94ea2cb6f24">https://developer.leapmotion.com/documentation/unity/class_leap_1_1_unity_1_1_interaction_1_1_interaction_behaviour.html#a33f9f48f2c6375cb926cc94ea2cb6f24</a> ]] for more details.</p>
<h3>Option 2: Use the OnGraspedMovement callback</h3>
<p>When grasped, objects fire their OnGraspedMovement callback <b>right after the Interaction Engine moves them with the grasping controller</b>. That means you can take advantage of this callback to <b>modify the Rigidbody position and/or rotation</b> just before PhysX performs its physics update. Setting up this callback will look something like this:</p>
<div class="fragment"><div class="line"><span class="keyword">private</span> InteractionBehaviour _intObj;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> Start() {</div><div class="line">  _intObj = GetComponent&lt;InteractionBehaviour&gt;();</div><div class="line">  _intObj.OnGraspedMovement += onGraspedMovement;</div><div class="line">}</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keywordtype">void</span> onGraspedMovement(Vector3 presolvedPos, Quaternion presolvedRot,</div><div class="line">                               Vector3 solvedPos,    Quaternion solvedRot,</div><div class="line">                               List&lt;InteractionController&gt; graspingControllers) {</div><div class="line">  <span class="comment">// Project the vector of the motion of the object due to grasping along the world X axis.</span></div><div class="line">  Vector3 movementDueToGrasp = solvedPos - presolvedPos;</div><div class="line">  <span class="keywordtype">float</span> xAxisMovement = movementDueToGrasp.x;</div><div class="line"></div><div class="line">  <span class="comment">// Move the object back to its position before the grasp solve this frame,</span></div><div class="line">  <span class="comment">// then add just its movement along the world X axis.</span></div><div class="line">  _intObj.rigidbody.position = presolvedPos;</div><div class="line">  _intObj.rigidbody.position += Vector3.right * xAxisMovement;</div><div class="line">}</div></div><!-- fragment --><h2>Constraining an interaction object's position and rotation generally</h2>
<p>The principles explained above for constraining a <b>held</b> interaction object's position and rotation also apply to constraining the interaction object's position and rotation even when it is not held. Of course, Rigidbody Joints will work as expected.</p>
<p>When scripting a custom constraint, however, instead of using the OnGraspedMovement callback, the Interaction Manager provides an OnPostPhysicalUpdate event that fires just after its FixedUpdate, in which it updates interaction controllers and interaction objects. This is would be a good place to apply your physical constraints.</p>
<div class="fragment"><div class="line"><span class="keyword">private</span> InteractionBehaviour _intObj;</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> Start() {</div><div class="line">  _intObj = GetComponent&lt;InteractionBehaviour&gt;();</div><div class="line">  _intObj.manager.OnPostPhysicalUpdate += applyXAxisWallConstraint;</div><div class="line">}</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keywordtype">void</span> applyXAxisWallConstraint() {</div><div class="line">  <span class="comment">// This constraint forces the interaction object to have a positive X coordinate.</span></div><div class="line">  Vector3 objPos = _intObj.rigidbody.position;</div><div class="line">  <span class="keywordflow">if</span> (objPos.x &lt; 0F) {</div><div class="line">    objPos.x = 0F;</div><div class="line">    _intObj.rigidbody.position = objPos;</div><div class="line"></div><div class="line">    <span class="comment">// Zero out any negative-X velocity when the constraint is applied.</span></div><div class="line">    Vector3 objVel = _intObj.rigidbody.velocity;</div><div class="line">    <span class="keywordflow">if</span> (objVel.x &lt; 0F) {</div><div class="line">      objVel = 0F;</div><div class="line">      _intObj.rigidbody.velocity = objVel;</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><h2>Applying forces to an interaction object</h2>
<p>If your interaction object is not actively being touched by an Interaction Hand or an Interaction VR Controller, you may apply forces to your Rigidbody using the standard API provided by Unity. However, when an object experiences external forces that press it into the user's controller or the user's hand, <b>the "soft contact" system provided by the Interaction Engine requires special knowledge of those external forces to properly account for them</b>. In any gameplay-critical circumstances involving forces of this nature, you should use the Forces API provided by interaction objects:</p>
<div class="fragment"><div class="line">_intObj.AddLinearAcceleration(myAccelerationAmount)</div><div class="line">_intObj.AddAngularAcceleration(myAngularAccelerationAmount)</div></div><!-- fragment --><p>These accelerations are ultimately applied using the Rigidbody forces API, but are also noted by the "soft contact" system, to prevent the object from nudging its way <em>through</em> any interaction controllers due to repeated application of these forces.</p>
<h1><a class="anchor" id="ie-in-depth"></a>
Interaction types in-depth</h1>
<h2>Hovering</h2>
<p>Hover functionality in the Interaction Engine consists of two inter-related subsystems, referred to as 'Hover' and 'Primary Hover' respectively.</p>
<h3>Proximity feedback ("Hover")</h3>
<p>Any interaction object within the Hover Activity Radius around an Interaction Controller's hover point will receive the OnHoverBegin, OnHoverStay, and OnHoverEnd callbacks and have its <code>isHovered</code> state set to true, as long as both the hovering controller and the interaction object have their hover settings enabled. Interaction objects provide a public getter for getting the closest hovering interaction controller as well. In general, hover information is useful when scripting visual and audio feedback related to proximity.</p>
<h3>Primary Hover</h3>
<p>Interaction controllers define one or more "primary hover points," and the closest interaction object (that is currently hovered by an interaction controller) to any of the interaction controller's primary hover points will become the primarily hovered object of that controller. This status can be queried at any time using a controller's <code>primaryHoveredObject</code> property or an interaction object's <code>isPrimaryHovered</code> property.</p>
<p>Fundamentally, primary hover is the feature that turns unreliable interfaces into reliable ones when <em>only the primary hovered object</em> of a given interaction controller can be depressed or otherwise interacted-with by that controller. This is why the button panel in [[Example 2 (Basic UI) | Getting-Started-(Interaction-Engine)example-2-basic-ui-in-the-interaction-engine]] will only depress one button per hand at any given time, even if you clumsily throw your whole hand into the panel. The InteractionButton, InteractionToggle, and InteractionSlider classes all implement this primary-hover-only strategy in order to produce more reliable interfaces.</p>
<h2>Contact</h2>
<p>Contact in the Interaction Engine consists of two subsystems:</p><ul>
<li><b>Contact Bones</b>, which are Rigidbodies with a single Collider and a ContactBone component that holds additional contact data for hands and controllers, and</li>
<li><b>Soft Contact</b>, which activates when Contact Bones get too dislocated from their target positions and rotations &ndash; in other words, when a hand or interaction controller jams itself too far "inside" an interaction object.</li>
</ul>
<h3>Contact Bones</h3>
<p>Interaction controller implementations are responsible for constructing and updating a set of GameObjects with Rigidbodies, Colliders, and ContactBone components, referred to as contact bones. The controller also is responsible for defining the "ideal" position and rotation for a given contact bone at all times. During the FixedUpdate, an interaction controller will set each of its contact bones' velocities and angular velocities such that the contact bone will reach its ideal position and rotation by the <em>next</em> FixedUpdate. These velocities then propagate through the Unity's physics engine (PhysX) update and may the bones may collide against objects in the scene, which will apply forces to them.</p>
<p>Additionally, at the beginning of every FixedUpdate, an interaction controller checks how dislocated a contact bone is from its intended position and rotation. If this dislocation becomes too large, the interaction controller will switch into Soft Contact mode, which effectively disables its contact bones by converting them into <a href="https://docs.unity3d.com/ScriptReference/Collider-isTrigger.html">Trigger</a> colliders.</p>
<h3>Soft Contact</h3>
<p>Soft Contact is essentially an alternative to the standard physical paradigm in physics engines of treating Rigidbodies as, well, perfectly rigid bodies. Instead, relative positions, rotations, velocities, and angular velocities are calculated as the trigger colliders of contact bones pass through the colliders of interaction objects, and custom velocities and angular velocities are applied each frame to any interaction objects that are colliding with the bones of an interaction controller in soft contact mode so that the controller and object will resist motions <em>deeper into</em> the object but freely allow motions <em>out of</em> the object.</p>
<p>If debug drawing is enabled on your Interaction Manager, you can tell when an interaction controller is in Soft Contact mode because its contact bones (by default) will render as white instead of green.</p>
<h2>Grasping</h2>
<p>When working with VR controllers, grasping is a pretty basic feature to implement: simply define which button should be used to grab objects, and use the motion of the grasp point to move any grasped object. However, when working with Leap hands, we no longer have the simplicity of dealing in digital buttons. Instead, we've implemented a finely-tuned heuristic for detecting when a user has intended to grasp an interaction object. Whether you're working with VR controllers or hands, the grasping API in the Interaction Engine provides a common interface for constructing logic around grasping, releasing, and throwing.</p>
<h3>Grasped pose &amp; object movement</h3>
<p>When an interaction controller picks up an object, the default implementation of all interaction controllers assumes that the intended behavior is for the object to follow the grasp point. Grasp points are explicit for InteractionVRControllers and are implicit for Interaction Hands, but the resulting behavior is the same in either case.</p>
<p>Objects are moved when held under one of two mutually exclusive movement modes: Kinematic, or Nonkinematic. By default, kinematic interaction objects will move kinematically when grasped, and nonkinematic interaction objects will move nonkinematically when grasped. When moving kinematically, an interaction object's rigidbody position and rotation <em>are set explicitly</em>, effectively teleporting the object to the new position and rotation. This allows the grasped object to clip through colliders it otherwise would not be able to penetrate. Nonkinematic grasping motions, however, cause an interaction object to instead <em>receive a velocity and angular velocity</em> that will move it to its new target position and rotation on the next physics engine update, which allows the object to collide against objects in the scene before reaching its target grasped position.</p>
<p>When an object is moved because it is being held by a moving controller, a [[special callback | Scripting-Interaction-Objectsoption-2-use-the-ongraspedmovement-callback]] is fired right after the object is moved, which you should subscribe to if you wish to modify how the object moves while it is grasped. Alternatively, you can disable the <code>moveObjectWhenGrasped</code> setting on interaction objects to prevent their grasped motion entirely (which will no longer cause the callback to fire).</p>
<h3>Throwing</h3>
<p>When a grasped object is released, its velocity and angular velocity are controlled by an object whose class implements the IThrowHandler interface. IThrowHandlers receive updates every frame during a grab so that they can accumulate velocity and angular velocity data about the object &ndash; most often, only the latest few frames of data are necessary. When the object is finally released, they get an OnThrow call, which in the default implementation (SlidingWindowThrow) sets the velocity of the object based on a recent historical average of the object's velocity while grasped. In practice, this results in greater intentionality in a user's throws.</p>
<p>However, if you'd like to create a different implementation of a throw, you can implement a new IThrowHandler and set the public <code>throwHandler</code> on any interaction object to change how throws behave.</p>
<h1><a class="anchor" id="interaction-engine-faq"></a>
FAQ</h1>
<p><b>Q: Can I translate and rotate my VR rig (player), say, on a moving ship, and still have Interaction Engine user interfaces work?</b></p>
<p>A: Yes, we support this via code in the InteractionManager that watches how its own Transform moves in between each FixedUpdate and translating the colliders in the player's Interaction Controllers accordingly. Refer to Example 7 (as of IE 1.1.0) to see a working implementation of this functionality! In general, make sure to:</p><ul>
<li>Translate and/or rotate the player <b>during FixedUpdate</b>, <b>before the InteractionManager performs its own FixedUpdate</b>. You can ensure your movement script occurs before the InteractionManager by setting its Script Execution Order to run before Default Time (<code>Edit/Project Settings/Script Execution Order</code>). Alternatively, an easy way to receive a callback to execute just before the Interaction Manager's FixedUpdate is to subscribe to the <code>interactionManager.OnPrePhysicalUpdate</code> event.</li>
<li>Make sure the Interaction Manager moves with the player when the player is translated or rotated. The easiest way to do this is to have the Interaction Manager be a child object of your player rig object, e.g., the <b>LMHeadMountedRig</b> object.</li>
</ul>
<p><b>Q: Will the Interaction Engine work at arbitrary player scales?</b></p>
<p>A: Currently, no. The Interaction Engine works best at "real-world" scale: <b>1 Unity distance unit = 1 real-world meter.</b> All of Leap Motion's Unity assets follow this rule, so you're fine if you keep our prefabs at unit scale. If you scale the player too far away from unit scale, certain interactions may stop functioning properly. We are working to support arbitrary interaction scales, but there is no timeline for this feature currently.</p>
<p><b>Q: How can I effectively grasp very small objects?</b></p>
<p>A: If you need to be able to grasp a very small object and the object's physical colliders don't produce good grasping behaviors, try adding a new primitive collider for the object, such as a SphereCollider, with a larger radius than the object itself, and with isTrigger set to true. As long as the InteractionBehaviour is not ignoring grasping, you will be able to grasp the object by this trigger volume.</p>
<p>However, you don't want a grasping-only trigger collider to be <em>too</em> much larger than the object itself. In general, the larger the grasping volume is around an object, the more likely the user is to accidentally grasp objects when they don't intend to. Additionally, having overlapping grasping-only trigger colliders from multiple objects will prevent the grasp classifier from correctly picking which object to grasp.</p>
<p><b>Q: How do I implement two-handed grasping?</b></p>
<p>A: You can check a checkbox named 'Allow Multi Grasp' that is located on the interaction object itself to enable two-handed grasp for that object. If you want to know if there are two hands currently grasping an object, you can use the <code>graspingHands</code> property of the InteractionBehaviour. This is a set of all hands currently grasping the object, so it will have a count of two if it's currently being grasped by two hands.</p>
<h1>Have a question not answered here?</h1>
<p>Head over to <a href="https://community.leapmotion.com/c/development" title="Leap Motion Developer Forums">the developer forum</a> and post your question! We'll see how we can help. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.11-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Thu May 31 2018 17:27:33 for Unity Modules by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.11 </li>
  </ul>
</div>
</body>
</html>
